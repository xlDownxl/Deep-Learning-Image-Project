{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Thursday, January 6, before 23:59.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Group Information\n",
    "\n",
    "Group name: deepbois <br>\n",
    "Group members: Niclas Joswig, Jonas De Paolis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Classification Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, inspect, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))\n",
    "image_dir = os.path.join(current_dir, 'train/images')\n",
    "label_dir = os.path.join(current_dir, 'train/annotations')\n",
    "aug_dir = os.path.join(current_dir, 'train_augmented/images/')\n",
    "pred_dir = os.path.join(current_dir, 'test/images/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB2GS(img):\n",
    "    \"\"\"Converts RGB image of shape (128,128,3) to grayscale image of shape (128,128,1).\"\"\"\n",
    "    \n",
    "    if(len(img.shape) == 3):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.expand_dims(img, axis=2)\n",
    "    return img\n",
    "\n",
    "def load_data(data_dir): \n",
    "    \"\"\"Load data from directory.\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        path = os.path.join(data_dir, file)\n",
    "        img = cv2.imread(path) # contains rgb encoded images\n",
    "        img = RGB2GS(img)\n",
    "        img_num = file.split(\".\")[0][2:] \n",
    "        data.append([img, int(img_num)]) \n",
    "    return data\n",
    "\n",
    "data = load_data(image_dir) \n",
    "img = np.array([i[0] for i in data])\n",
    "img_number = np.array([int(i[1]) for i in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotation():\n",
    "    \"\"\"Load annotations from directory.\"\"\"\n",
    "    \n",
    "    annotation = []\n",
    "    for file in sorted(os.listdir(label_dir)):\n",
    "        text_file = open(os.path.join(label_dir, file), \"r\")\n",
    "        lines = text_file.read().split('\\n')\n",
    "        annotation.append(np.array(lines))\n",
    "        text_file.close()\n",
    "    annotation = np.array(annotation)\n",
    "    return annotation\n",
    "\n",
    "def build_multilabel_arr(img_number, annotation):\n",
    "    \"\"\"Build multilabel array of shape (n_points, n_classes).\"\"\"\n",
    "    \n",
    "    n_points = len(img_number)\n",
    "    n_classes = len(annotation)\n",
    "    arr = np.zeros((n_points, n_classes))\n",
    "    for i in range(n_points):        \n",
    "        for j in range(n_classes):\n",
    "            if np.any(str(img_number[i]) == annotation[j]):\n",
    "                arr[i,j] = 1\n",
    "    return arr\n",
    "\n",
    "annotation = load_annotation()\n",
    "lab = build_multilabel_arr(img_number, annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define class weights. Note: we ended up not using these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(annotation):\n",
    "    \"\"\"Compute class weights based on number of occurences.\"\"\"\n",
    "    \n",
    "    occurences = [] # count occurences\n",
    "    weights = [] # compute weights\n",
    "    \n",
    "    for i in range(len(annotation)):\n",
    "        occurences.append(len(annotation[i]))\n",
    "    for i in range(len(annotation)):\n",
    "        weights.append(max(occurences)/occurences[i])\n",
    "        \n",
    "    indices = list(range(14)) # build dictionary\n",
    "    class_weights = dict(zip(indices, weights))\n",
    "    return class_weights\n",
    "\n",
    "class_weights = get_class_weights(annotation)\n",
    "\n",
    "# largest class: 9 (reference), smallest class: 0\n",
    "for i in class_weights:\n",
    "    print(f\"class {i}: {class_weights[i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COPIES = 2 # number of new copies per original image\n",
    "\n",
    "# instantiate image generator\n",
    "aug_generator = ImageDataGenerator(\n",
    "    rotation_range=10, \n",
    "    width_shift_range=0.1, \n",
    "    height_shift_range=0.1, \n",
    "    horizontal_flip=True, \n",
    "    fill_mode='reflect')\n",
    "    \n",
    "def create_augdata(generator, save_files=False):\n",
    "    \"\"\"Create augmented data including images and labels.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(aug_dir) and save_files:\n",
    "        os.makedirs(aug_dir)\n",
    "    \n",
    "    n_images = len(img)\n",
    "    aug_images = []\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        image = np.expand_dims(img[i], axis=0)\n",
    "        image_number = img_number[i]\n",
    "        \n",
    "        # get new images from generator\n",
    "        loader = generator.flow(image)\n",
    "        image_versions = [next(loader)[0].reshape(128,128).astype(np.uint8) for i in range(N_COPIES)]\n",
    "        image_version_numbers = [f\"{image_number}_{version}\" for version in range(N_COPIES)]\n",
    "        aug_images.append(image_versions)\n",
    "        \n",
    "        # save files to directory\n",
    "        if save_files:\n",
    "            for file, filename in zip(image_versions, image_version_numbers):\n",
    "                file = Image.fromarray(file)\n",
    "                file.save(aug_dir+filename+\".jpg\")\n",
    "    \n",
    "    aug_images = np.array(aug_images).reshape(n_images*N_COPIES,128,128,1)\n",
    "    aug_labels = np.repeat(lab, N_COPIES, axis=0)\n",
    "    return aug_images, aug_labels\n",
    "    \n",
    "img_augmented, lab_augmented = create_augdata(aug_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate original with augmented data and shuffle both X and y in unison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate original data and augmented data\n",
    "X_data = np.vstack((img, img_augmented))\n",
    "y_data = np.vstack((lab, lab_augmented))\n",
    "\n",
    "def shuffle_data(X_data, y_data):\n",
    "    \"\"\"Shuffle both X and y in unison.\"\"\"\n",
    "    \n",
    "    assert len(X_data) == len(y_data)\n",
    "    perm = np.random.permutation(len(X_data))\n",
    "    return X_data[perm], y_data[perm]\n",
    "\n",
    "X_data, y_data = shuffle_data(X_data, y_data)\n",
    "\n",
    "### use smaller sample\n",
    "DATA_POINTS = 100\n",
    "X_data = X_data[:DATA_POINTS]\n",
    "y_data = y_data[:DATA_POINTS]\n",
    "\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Automated Model Optimisation \n",
    "\n",
    "... using random search. \n",
    "Note: we ran this particular code on macos, with other systems we sometimes experienced complications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement F1 score metric. <br>\n",
    "Note: this code is copied from https://stackoverflow.com/a/45305384."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    \n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        \n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings for random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONV = 4 # max number of convolutional layers\n",
    "MAX_DENSE = 4 # max number of dense layers\n",
    "N_FOLDS = 5 # number of folds used for cross validation (>=2)\n",
    "N_ITER = 30 # number of tested hyperparameter combinations (N_ITER * CV = number of runs)\n",
    "N_CORES = 1 # note: do NOT use -1 for all cores\n",
    "VERBOSE_PROGRESS = 1 # ...\n",
    "VERBOSE_CONFIG = 2 # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define architecture template. Fixed elements are output activation function (sigmoid) and\n",
    "loss function (binary crossentropy). Since we will optimise most of the hidden layer hyperparameters later on through `RandomizedSearchCV`, those concrete values are replaced by variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (128, 128, 1) # image size is 128x128 with one channel\n",
    "\n",
    "def create_model(\n",
    "    conv_n, conv_activation, kern_n, kern_size, kern_stride,\n",
    "    dense_n, dense_activation, dense_init, unit_n, dropout,\n",
    "    epochs, batch_size, optimizer):\n",
    "    \"\"\"Build CNN architecture.\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # add convolutional layers\n",
    "    for i in range(conv_n):\n",
    "        \n",
    "        if i==0: model.add(Conv2D(activation=conv_activation, filters=kern_n[i], kernel_size=kern_size[i], strides=kern_stride[i], padding='same', input_shape=INPUT_SHAPE))\n",
    "        else: model.add(Conv2D(activation=conv_activation, filters=kern_n[i], kernel_size=kern_size[i], strides=kern_stride[i], padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    # flatten for transition to dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # add dense layers\n",
    "    for i in range(dense_n):\n",
    "        model.add(Dense(activation=dense_activation, kernel_initializer=dense_init, units=unit_n[i]))\n",
    "        model.add(Dropout(rate=dropout[i]))\n",
    "    \n",
    "    # output layer is fixed: 14 classes, sigmoid activation\n",
    "    model.add(Dense(activation='sigmoid', units=14)) \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc', f1]) # f1 metric as implemented above\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define random search optimisation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_params(params, n):\n",
    "    \"\"\"Create combinations of parameters for n layers.\"\"\"\n",
    "    \n",
    "    params = tuple(params)\n",
    "    combinations = product(params, repeat=n)                           \n",
    "    return [c for c in combinations]\n",
    "\n",
    "def optimise_model(model, params, data):\n",
    "    \"\"\"Perform random search and return optimal parameters.\"\"\"\n",
    "    \n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=params, refit=True, n_iter=N_ITER, cv=N_FOLDS, n_jobs=N_CORES, verbose=VERBOSE_CONFIG) \n",
    "    random_search = random_search.fit(X=data[0], y=data[1]) # class_weight=class_weights\n",
    "    \n",
    "    best_score = random_search.best_score_\n",
    "    best_params = random_search.best_params_\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    results = random_search.cv_results_\n",
    "    return best_score, best_params, best_model, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameter space that is to be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of hyperparameter options\n",
    "parameters = {\n",
    "    \n",
    "    # conv layers\n",
    "    'conv_n'            : np.arange(1, MAX_CONV+1),\n",
    "    'conv_activation'   : ['relu', 'tanh'],\n",
    "    'kern_n'            : combine_params([32, 64, 128], n=MAX_CONV),\n",
    "    'kern_size'         : combine_params([3, 4, 5], n=MAX_CONV),\n",
    "    'kern_stride'       : combine_params([1], n=MAX_CONV),\n",
    "    \n",
    "    # dense layers\n",
    "    'dense_n'           : np.arange(1, MAX_DENSE+1),\n",
    "    'dense_activation'  : ['relu', 'tanh'],\n",
    "    'dense_init'        : ['truncated_normal', 'glorot_uniform'],\n",
    "    'unit_n'            : combine_params([32, 128, 512], n=MAX_DENSE),\n",
    "    'dropout'           : combine_params([0.3, 0.4, 0.5], n=MAX_DENSE),\n",
    "    \n",
    "    # other\n",
    "    'epochs'            : [10, 20], \n",
    "    'batch_size'        : [16, 128, 256],\n",
    "    'optimizer'         : ['adam', 'rmsprop']\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and run random search\n",
    "cnn_prototype = KerasClassifier(build_fn=create_model, verbose=VERBOSE_PROGRESS)\n",
    "best_score, best_params, best_model, results = optimise_model(model=cnn_prototype, params=parameters, data=[X_data, y_data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the top 3 architectures found during random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display N best architectures\n",
    "N = 3\n",
    "\n",
    "# create and sort df by test score rank\n",
    "results = pd.DataFrame(results)\n",
    "results = results.sort_values('rank_test_score')\n",
    "results = results.reset_index(drop=True)\n",
    "\n",
    "# print scores and architectures\n",
    "for n in range(N):\n",
    "    \n",
    "    rank_n_best = results.loc[n, 'rank_test_score']\n",
    "    print(f\"\\n> Test score rank: {rank_n_best}\")\n",
    "    \n",
    "    score_n_best = results.loc[n, 'mean_test_score']\n",
    "    print(f\"> Test score: {score_n_best}\\n\")\n",
    "    \n",
    "    params_n_best = results.loc[n, 'params']\n",
    "    model_n_best = best_model.set_params(**params_n_best)\n",
    "    print(best_model.model.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Manual Model Optimisation\n",
    "\n",
    "Based on the collected metadata, optimise the best of the aforementioned architectures by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# generate train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below given code represents the final and best architecture we could come up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Build CNN architecture.\"\"\"\n",
    "    \n",
    "    # conv2d layers ...\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), border_mode='same', activation='relu', kernel_initializer='truncated_normal', input_shape=(128,128,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), border_mode='same', activation='relu', kernel_initializer='truncated_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), border_mode='same', activation='relu', kernel_initializer='truncated_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(256, (3,3), border_mode='same', activation='relu', kernel_initializer='truncated_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # dense layers ...\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=\"truncated_normal\"))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(14, activation='sigmoid', kernel_initializer=\"truncated_normal\"))\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy', f1], optimizer='adam')\n",
    "    return model\n",
    "\n",
    "cnn_final = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model, generate plot and save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = cnn_final.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32, shuffle=True, verbose=2)\n",
    "\n",
    "# plot history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss'); plt.ylabel('loss'); plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot history for F1\n",
    "plt.plot(history.history['f1'])\n",
    "plt.plot(history.history['val_f1'])\n",
    "plt.title('model F1'); plt.ylabel('F1'); plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# versioning\n",
    "import datetime\n",
    "time = datetime.datetime.now()\n",
    "timestamp = time.strftime('%y%m%d_%H%M%S')\n",
    "\n",
    "# save model\n",
    "cnn_final.save(f'./_results/model_cnn_{timestamp}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# cnn_final = load_model('./_results/_.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best threshold for each class. For details, check project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds = cnn_final.predict(X_test)\n",
    "\n",
    "# ...\n",
    "threshold_options = [.1, .15, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "best_thresholds = []\n",
    "\n",
    "# for each target variable ...\n",
    "for target_var in range(14):\n",
    "    \n",
    "    # compute score for each threshold\n",
    "    scores = []\n",
    "    for threshold in threshold_options:\n",
    "        \n",
    "        # first, use threshold option for observed class\n",
    "        preds_ = preds.copy()\n",
    "        preds_target = preds_[:, target_var]\n",
    "        preds_target[preds_target > threshold] = 1\n",
    "        preds_target[preds_target <= threshold] = 0\n",
    "        preds_[:, target_var] = preds_target\n",
    "        \n",
    "        # then, use default threshold (0.2, overall best for all classes) for all other classes\n",
    "        preds_[preds_ > 0.2] = 1\n",
    "        preds_[preds_ <= 0.2] = 0\n",
    "        \n",
    "        # compute micro F1 score\n",
    "        score = f1_score(y_test.astype(int), preds_, average='micro')  \n",
    "        scores.append(score)\n",
    "    \n",
    "    # find best threshold\n",
    "    best_score = max(scores)\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] == best_score:\n",
    "            threshold = threshold_options[i]\n",
    "            \n",
    "    # append to list\n",
    "    best_thresholds.append(threshold)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the best performing thresholds per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_THRESHOLDS = [.1, .9, .6, .4, .1, .3, .4, .3, .4, .3, .3, .9, .3, .4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use final model, best thresholds and validation data to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds = cnn_final.predict(X_test)\n",
    "\n",
    "# for each target variable ...\n",
    "for target_var in range(14):\n",
    "\n",
    "    preds_target = preds[:, target_var]\n",
    "    preds_target[preds_target > BEST_THRESHOLDS[i]] = 1\n",
    "    preds_target[preds_target <= BEST_THRESHOLDS[i]] = 0\n",
    "    preds[:, target_var] = preds_target\n",
    "\n",
    "score = f1_score(y_test, preds, average='micro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image data that is to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(pred_dir) \n",
    "X_pred = np.array([i[0] for i in data])\n",
    "print(X_pred.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds = cnn_final.predict(X_pred)\n",
    "\n",
    "# for each target variable ...\n",
    "for target_var in range(14):\n",
    "\n",
    "    preds_target = preds[:, target_var]\n",
    "    preds_target[preds_target > BEST_THRESHOLDS[i]] = 1\n",
    "    preds_target[preds_target <= BEST_THRESHOLDS[i]] = 0\n",
    "    preds[:, target_var] = preds_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save prediction output matrix to .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./_results/results.txt', preds, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
